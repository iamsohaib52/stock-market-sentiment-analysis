{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3f63b9a",
   "metadata": {},
   "source": [
    "### Import Libraries, Download NLTK Resources, and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cff2eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\FLH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\FLH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset shape: (5791, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kickers on my watchlist XIDE TIT SOQ PNK CPW B...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user: AAP MOVIE. 55% return for the FEA/GEED i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user I'd be afraid to short AMZN - they are lo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MNTA Over 12.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OI  Over 21.37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  Kickers on my watchlist XIDE TIT SOQ PNK CPW B...          1\n",
       "1  user: AAP MOVIE. 55% return for the FEA/GEED i...          1\n",
       "2  user I'd be afraid to short AMZN - they are lo...          1\n",
       "3                                  MNTA Over 12.00            1\n",
       "4                                   OI  Over 21.37            1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP & ML\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Gensim for Word2Vec embeddings\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Ensure NLTK resources are available\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load dataset\n",
    "DATA_PATH = Path('stock_data.csv')\n",
    "assert DATA_PATH.exists(), f\"Dataset not found at {DATA_PATH}. Please check path.\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH, encoding='utf-8')\n",
    "\n",
    "# Rename columns consistently (safe-guard)\n",
    "if list(df.columns)[:2] != ['review', 'sentiment']:\n",
    "    # Try to detect common names\n",
    "    cols = df.columns.tolist()\n",
    "    # If first column looks textual rename to review, second to sentiment\n",
    "    df = df.rename(columns={cols[0]: 'review', cols[1]: 'sentiment'})\n",
    "\n",
    "print(\"Loaded dataset shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfdc23f",
   "metadata": {},
   "source": [
    "### Text Cleaning, Tokenization, and Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b894ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example cleaned review: kickers on my watchlist xide tit soq pnk cpw bpz aj trade method 1 or method 2 see prev posts\n",
      "Example tokens: ['kickers', 'watchlist', 'xide', 'tit', 'soq', 'pnk', 'cpw', 'bpz', 'aj', 'trade', 'method', 'method', 'see', 'prev', 'posts']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kickers on my watchlist xide tit soq pnk cpw b...</td>\n",
       "      <td>1</td>\n",
       "      <td>[kickers, watchlist, xide, tit, soq, pnk, cpw,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user aap movie 55 return for the fea geed indi...</td>\n",
       "      <td>1</td>\n",
       "      <td>[user, aap, movie, return, fea, geed, indicato...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user i d be afraid to short amzn they are look...</td>\n",
       "      <td>1</td>\n",
       "      <td>[user, afraid, short, amzn, looking, like, nea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mnta over 12 00</td>\n",
       "      <td>1</td>\n",
       "      <td>[mnta]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oi over 21 37</td>\n",
       "      <td>1</td>\n",
       "      <td>[oi]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  \\\n",
       "0  kickers on my watchlist xide tit soq pnk cpw b...          1   \n",
       "1  user aap movie 55 return for the fea geed indi...          1   \n",
       "2  user i d be afraid to short amzn they are look...          1   \n",
       "3                                    mnta over 12 00          1   \n",
       "4                                      oi over 21 37          1   \n",
       "\n",
       "                                              tokens  \n",
       "0  [kickers, watchlist, xide, tit, soq, pnk, cpw,...  \n",
       "1  [user, aap, movie, return, fea, geed, indicato...  \n",
       "2  [user, afraid, short, amzn, looking, like, nea...  \n",
       "3                                             [mnta]  \n",
       "4                                               [oi]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removes HTML, URLs, punctuation, extra whitespace, lowercases, and removes stopwords.\n",
    "# Keeps the tokenized words in a helper column for Word2Vec training. \n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove HTML, URLs, punctuation; lowercase; strip repeated whitespace.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    # Remove HTML\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text(separator=\" \")\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove non-ascii characters\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    # Remove punctuation (keep periods replaced by space)\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    # Lowercase and collapse whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "    return text\n",
    "\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in STOPWORDS]\n",
    "    return tokens\n",
    "\n",
    "# Apply cleaning and tokenization\n",
    "df['review'] = df['review'].astype(str).apply(clean_text)\n",
    "df['tokens'] = df['review'].apply(tokenize_and_remove_stopwords)\n",
    "\n",
    "# Display quick checks\n",
    "print(\"Example cleaned review:\", df['review'].iloc[0])\n",
    "print(\"Example tokens:\", df['tokens'].iloc[0])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04badc9",
   "metadata": {},
   "source": [
    "### Normalize Sentiment Labels and Show Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "351ea5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      "sentiment\n",
      "Positive    3685\n",
      "Negative    2106\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total examples: 5791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5791, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map original numeric labels to 'Positive'/'Negative' if necessary and show counts.\n",
    "\n",
    "# Map if numeric labels exist (1 / -1)\n",
    "if df['sentiment'].dtype.kind in 'biufc':  # numeric\n",
    "    df['sentiment'] = df['sentiment'].replace({1: 'Positive', -1: 'Negative'})\n",
    "\n",
    "# Ensure labels are strings\n",
    "df['sentiment'] = df['sentiment'].astype(str)\n",
    "\n",
    "print(\"Class distribution:\")\n",
    "print(df['sentiment'].value_counts())\n",
    "print(\"\\nTotal examples:\", len(df))\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea934e3e",
   "metadata": {},
   "source": [
    "### Feature Extraction: TF-IDF and Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ff412ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF feature matrix shape: (5791, 7525)\n",
      "Word2Vec averaged matrix shape: (5791, 100)\n"
     ]
    }
   ],
   "source": [
    "# Create TF-IDF features (for deployment) and train a small Word2Vec model to create averaged embeddings\n",
    "# (for research/comparison). We use ngram_range=(1,2) by default for TF-IDF.\n",
    "\n",
    "# Prepare raw text for TF-IDF (joined tokens)\n",
    "df['clean_text'] = df['tokens'].apply(lambda toks: ' '.join(toks))\n",
    "\n",
    "# TF-IDF vectorizer (this will be saved for the Flask app)\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=2, max_df=0.9, ngram_range=(1,2), norm='l2')\n",
    "\n",
    "# Fit TF-IDF on the full dataset (so deploy vectorizer knows full vocab)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['clean_text'])\n",
    "print(\"TF-IDF feature matrix shape:\", X_tfidf.shape)\n",
    "\n",
    "# Train a Word2Vec model on token lists (small dimensions so it's lightweight)\n",
    "w2v_size = 100\n",
    "w2v_window = 5\n",
    "w2v_min_count = 2\n",
    "\n",
    "w2v_model = Word2Vec(sentences=df['tokens'].tolist(),\n",
    "                     vector_size=w2v_size,\n",
    "                     window=w2v_window,\n",
    "                     min_count=w2v_min_count,\n",
    "                     workers=2,\n",
    "                     epochs=25)\n",
    "\n",
    "# Function to get average Word2Vec vector for a document\n",
    "def document_vector(tokens, w2v_model, size=w2v_size):\n",
    "    vecs = []\n",
    "    for t in tokens:\n",
    "        if t in w2v_model.wv:\n",
    "            vecs.append(w2v_model.wv[t])\n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros(size, dtype=np.float32)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "# Build matrix of averaged embeddings\n",
    "X_w2v = np.vstack(df['tokens'].apply(lambda toks: document_vector(toks, w2v_model)).values)\n",
    "print(\"Word2Vec averaged matrix shape:\", X_w2v.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b708247e",
   "metadata": {},
   "source": [
    "### Train/Test Split and Model Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c3a2bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create consistent train/test splits (stratified) and a small evaluation util to print metrics.\n",
    "\n",
    "# Features and target\n",
    "y = df['sentiment']\n",
    "\n",
    "# We'll make two different feature sets and share the same train/test split indices for fair comparison\n",
    "X_tfidf_full = X_tfidf\n",
    "X_w2v_full = X_w2v\n",
    "\n",
    "# Stratified split\n",
    "X_tfidf_train, X_tfidf_test, X_w2v_train, X_w2v_test, y_train, y_test = train_test_split(\n",
    "    X_tfidf_full, X_w2v_full, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "# Note: X_tfidf_train/test are sparse matrices; X_w2v_train/test are dense numpy arrays\n",
    "\n",
    "def evaluate_model(name, y_true, y_pred, y_prob=None, labels=None):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    print(f\"--- {name} ---\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision (weighted): {p:.4f}\")\n",
    "    print(f\"Recall (weighted):    {r:.4f}\")\n",
    "    print(f\"F1-score (weighted):  {f:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    # Try ROC AUC if binary\n",
    "    if y_prob is not None:\n",
    "        try:\n",
    "            # binarize labels if necessary\n",
    "            lb = LabelBinarizer()\n",
    "            y_bin = lb.fit_transform(y_test)\n",
    "            if y_bin.shape[1] == 1:\n",
    "                # binary classifier\n",
    "                auc = roc_auc_score(y_bin, y_prob[:,1] if y_prob.ndim>1 else y_prob)\n",
    "                print(\"ROC AUC:\", auc)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebd4c1f",
   "metadata": {},
   "source": [
    "### Train and Evaluate Models Using TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eab1e0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LogisticRegression (TF-IDF) ---\n",
      "Accuracy: 0.7791\n",
      "Precision (weighted): 0.7936\n",
      "Recall (weighted):    0.7791\n",
      "F1-score (weighted):  0.7602\n",
      "Confusion Matrix:\n",
      "[[200 221]\n",
      " [ 35 703]]\n",
      "ROC AUC: 0.8662785083907846\n",
      "\n",
      "--- MultinomialNB (TF-IDF) ---\n",
      "Accuracy: 0.7739\n",
      "Precision (weighted): 0.7898\n",
      "Recall (weighted):    0.7739\n",
      "F1-score (weighted):  0.7532\n",
      "Confusion Matrix:\n",
      "[[193 228]\n",
      " [ 34 704]]\n",
      "ROC AUC: 0.8636038854450303\n",
      "\n",
      "--- LinearSVC (TF-IDF) ---\n",
      "Accuracy: 0.7929\n",
      "Precision (weighted): 0.7898\n",
      "Recall (weighted):    0.7929\n",
      "F1-score (weighted):  0.7884\n",
      "Confusion Matrix:\n",
      "[[267 154]\n",
      " [ 86 652]]\n",
      "\n",
      "TF-IDF models F1 scores (weighted): {'logistic': 0.7601658557553608, 'mnb': 0.7532345369019734, 'svc': 0.7883885577518115}\n",
      "Best TF-IDF model: svc\n"
     ]
    }
   ],
   "source": [
    "# Train LogisticRegression, MultinomialNB, and LinearSVC on TF-IDF features.\n",
    "# For speed, we use reasonable defaults. We print report outputs for each model and keep them in a dictionary.\n",
    "\n",
    "models_tfidf = {}\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression(penalty='l2', C=1.0, max_iter=1000, random_state=42)\n",
    "lr.fit(X_tfidf_train, y_train)\n",
    "y_pred_lr = lr.predict(X_tfidf_test)\n",
    "# Obtain probability if available (LogisticRegression supports predict_proba)\n",
    "y_prob_lr = lr.predict_proba(X_tfidf_test) if hasattr(lr, \"predict_proba\") else None\n",
    "evaluate_model(\"LogisticRegression (TF-IDF)\", y_test, y_pred_lr, y_prob_lr)\n",
    "models_tfidf['logistic'] = lr\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "mnb = MultinomialNB()\n",
    "# MultinomialNB requires non-negative input; TF-IDF is non-negative\n",
    "mnb.fit(X_tfidf_train, y_train)\n",
    "y_pred_mnb = mnb.predict(X_tfidf_test)\n",
    "y_prob_mnb = mnb.predict_proba(X_tfidf_test) if hasattr(mnb, \"predict_proba\") else None\n",
    "evaluate_model(\"MultinomialNB (TF-IDF)\", y_test, y_pred_mnb, y_prob_mnb)\n",
    "models_tfidf['mnb'] = mnb\n",
    "\n",
    "# Linear SVM (LinearSVC)\n",
    "svc = LinearSVC(max_iter=5000, random_state=42)\n",
    "svc.fit(X_tfidf_train, y_train)\n",
    "y_pred_svc = svc.predict(X_tfidf_test)\n",
    "# LinearSVC does not have predict_proba by default\n",
    "evaluate_model(\"LinearSVC (TF-IDF)\", y_test, y_pred_svc)\n",
    "models_tfidf['svc'] = svc\n",
    "\n",
    "# Summarize weighted F1-scores to pick best\n",
    "from sklearn.metrics import f1_score\n",
    "f1_scores = {\n",
    "    'logistic': f1_score(y_test, y_pred_lr, average='weighted'),\n",
    "    'mnb': f1_score(y_test, y_pred_mnb, average='weighted'),\n",
    "    'svc': f1_score(y_test, y_pred_svc, average='weighted'),\n",
    "}\n",
    "print(\"TF-IDF models F1 scores (weighted):\", f1_scores)\n",
    "best_tfidf_key = max(f1_scores, key=f1_scores.get)\n",
    "print(\"Best TF-IDF model:\", best_tfidf_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7824796",
   "metadata": {},
   "source": [
    "### Train and Evaluate Models Using Averaged Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48bc53fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LogisticRegression (Word2Vec avg) ---\n",
      "Accuracy: 0.7058\n",
      "Precision (weighted): 0.7003\n",
      "Recall (weighted):    0.7058\n",
      "F1-score (weighted):  0.6777\n",
      "Confusion Matrix:\n",
      "[[151 270]\n",
      " [ 71 667]]\n",
      "\n",
      "--- LinearSVC (Word2Vec avg) ---\n",
      "Accuracy: 0.7248\n",
      "Precision (weighted): 0.7258\n",
      "Recall (weighted):    0.7248\n",
      "F1-score (weighted):  0.6985\n",
      "Confusion Matrix:\n",
      "[[162 259]\n",
      " [ 60 678]]\n",
      "\n",
      "Word2Vec models F1 scores (weighted): {'logistic': 0.6777298721298297, 'svc': 0.6985214932827438}\n",
      "Best Word2Vec model: svc\n"
     ]
    }
   ],
   "source": [
    "# Same model types as TF-IDF, but on the averaged embedding features.\n",
    "\n",
    "models_w2v = {}\n",
    "\n",
    "# Logistic Regression on embeddings (dense input)\n",
    "lr_e = LogisticRegression(penalty='l2', C=1.0, max_iter=1000, random_state=42)\n",
    "lr_e.fit(X_w2v_train, y_train)\n",
    "y_pred_lr_e = lr_e.predict(X_w2v_test)\n",
    "evaluate_model(\"LogisticRegression (Word2Vec avg)\", y_test, y_pred_lr_e)\n",
    "models_w2v['logistic'] = lr_e\n",
    "\n",
    "# MultinomialNB is not ideal for dense embeddings (expects counts) — skip MNB for embeddings.\n",
    "# Use LinearSVC instead\n",
    "svc_e = LinearSVC(max_iter=5000, random_state=42)\n",
    "svc_e.fit(X_w2v_train, y_train)\n",
    "y_pred_svc_e = svc_e.predict(X_w2v_test)\n",
    "evaluate_model(\"LinearSVC (Word2Vec avg)\", y_test, y_pred_svc_e)\n",
    "models_w2v['svc'] = svc_e\n",
    "\n",
    "# Summarize F1 scores\n",
    "f1_scores_w2v = {\n",
    "    'logistic': f1_score(y_test, y_pred_lr_e, average='weighted'),\n",
    "    'svc': f1_score(y_test, y_pred_svc_e, average='weighted'),\n",
    "}\n",
    "print(\"Word2Vec models F1 scores (weighted):\", f1_scores_w2v)\n",
    "best_w2v_key = max(f1_scores_w2v, key=f1_scores_w2v.get)\n",
    "print(\"Best Word2Vec model:\", best_w2v_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c2275c",
   "metadata": {},
   "source": [
    "### Select and Save Best Model Pipeline for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0743ca2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected TF-IDF pipeline for deployment: svc\n",
      "Saved deployment artifacts to webapp\n"
     ]
    }
   ],
   "source": [
    "# We will pick the best TF-IDF model (based on F1) for deployment with Flask.\n",
    "# Additionally, we save the TF-IDF vectorizer, the chosen model, and a README meta file in webapp/\n",
    "\n",
    "WEBAPP_DIR = Path('webapp')\n",
    "WEBAPP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Choose by comparing best TF-IDF F1 vs best W2V F1\n",
    "best_overall = None\n",
    "if max(f1_scores.values()) >= max(f1_scores_w2v.values()):\n",
    "    # Deploy TF-IDF best model\n",
    "    best_overall = ('tfidf', best_tfidf_key, models_tfidf[best_tfidf_key])\n",
    "    # Save vectorizer and model\n",
    "    with open(WEBAPP_DIR / 'vectorizer_tfidf.pkl', 'wb') as f:\n",
    "        pickle.dump(tfidf_vectorizer, f)\n",
    "    with open(WEBAPP_DIR / 'model_tfidf.pkl', 'wb') as f:\n",
    "        pickle.dump(models_tfidf[best_tfidf_key], f)\n",
    "    print(f\"Selected TF-IDF pipeline for deployment: {best_tfidf_key}\")\n",
    "else:\n",
    "    # Deploy Word2Vec embedding pipeline (slower at runtime but available)\n",
    "    best_overall = ('w2v', best_w2v_key, models_w2v[best_w2v_key])\n",
    "    # Save W2V model and classifier\n",
    "    w2v_model.save(str(WEBAPP_DIR / 'w2v_model.model'))\n",
    "    with open(WEBAPP_DIR / 'model_w2v.pkl', 'wb') as f:\n",
    "        pickle.dump(models_w2v[best_w2v_key], f)\n",
    "    print(f\"Selected Word2Vec pipeline for deployment: {best_w2v_key}\")\n",
    "\n",
    "# Save a small meta file describing which pipeline was chosen\n",
    "meta = {\n",
    "    'deployed_pipeline': best_overall[0],\n",
    "    'deployed_model_key': best_overall[1],\n",
    "    'notes': \"TF-IDF pipeline is recommended for fast web deployment. Word2Vec is available for research comparison.\"\n",
    "}\n",
    "with open(WEBAPP_DIR / 'deployment_meta.pkl', 'wb') as f:\n",
    "    pickle.dump(meta, f)\n",
    "\n",
    "print(\"Saved deployment artifacts to\", WEBAPP_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bbc597",
   "metadata": {},
   "source": [
    "### Sanity Check: Local Prediction Test with Saved Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d6280f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo sentence: Shares surge after the company reports record profits and strong guidance.\n",
      "Predicted sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "# Demonstrates how the Flask app will call model + vectorizer. We do it here for confirmation.\n",
    "\n",
    "def predict_text_tf(text, vectorizer, model):\n",
    "    txt = clean_text(text)\n",
    "    tokens = tokenize_and_remove_stopwords(txt)\n",
    "    joined = ' '.join(tokens)\n",
    "    Xv = vectorizer.transform([joined])\n",
    "    pred = model.predict(Xv)[0]\n",
    "    prob = model.predict_proba(Xv)[0] if hasattr(model, \"predict_proba\") else None\n",
    "    return pred, prob\n",
    "\n",
    "# Load saved artifacts for demo (choose whichever exists)\n",
    "if (WEBAPP_DIR / 'vectorizer_tfidf.pkl').exists():\n",
    "    with open(WEBAPP_DIR / 'vectorizer_tfidf.pkl', 'rb') as f:\n",
    "        loaded_vectorizer = pickle.load(f)\n",
    "    with open(WEBAPP_DIR / 'model_tfidf.pkl', 'rb') as f:\n",
    "        loaded_model = pickle.load(f)\n",
    "    demo_sent = \"Shares surge after the company reports record profits and strong guidance.\"\n",
    "    pred, prob = predict_text_tf(demo_sent, loaded_vectorizer, loaded_model)\n",
    "    print(\"Demo sentence:\", demo_sent)\n",
    "    print(\"Predicted sentiment:\", pred)\n",
    "    if prob is not None:\n",
    "        print(\"Prediction probabilities:\", prob)\n",
    "else:\n",
    "    print(\"TF-IDF deployment artifacts not found — check previous cell outputs.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
